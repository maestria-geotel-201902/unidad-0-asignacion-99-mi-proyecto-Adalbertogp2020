---
# output: github_document
output: 
  pdf_document:
    keep_tex: true
    fig_caption: true
    latex_engine: pdflatex
    template: svm-latex-ms.tex
    number_sections: true
title: | 
        | Mi proyecto
        | Vecindad y autocorrelación espacial
        | Proyecto Final de Analis Espacial
        | Profesor :Jose Ramon Martinez Batlle 
author:
- name: Maria Magdalena Viloria y Adalberto Guerrero P.
  affiliation: Estudiante, Universidad Autónoma de Santo Domingo (UASD)
abstract: "Mi resumen"
keywords: "palabra clave 1, palabra clave 2"
date: "`r format(Sys.time(), '%B %d, %Y')`"
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
# spacing: double
bibliography: bibliography.bib
# csl: plos-one.csl
csl: apa.csl
header-includes:
  \usepackage{pdflscape}
  \newcommand{\blandscape}{\begin{landscape}}
  \newcommand{\elandscape}{\end{landscape}}
---


# Introducción

'A que cree que se debe la delincuencia en el pais ¿A la falta de alternativas sanas (clubes, cine, teatro, etc.) para el entretenimiento?: Si'

# Metodología
El objetivo de este proyecto es el análisis exploratorio de datos espaciales, funciones de homogeneidad espacial, tipos de vecindad, ponderadores y con el índice de autocorrelación espacial de Moran.

Usaremos como referencia la capa de provincias dominicanas y los resultados de la Encuesta Nacional de Hogares de Propósitos Múltiples de 2017 (ENHOGAR-2017, de. Te usaremos la pregunta de investigacion, sobre la cual sera realizado el análisis de autocorrelación al final.



\ldots
## Paquetes

* Carga el paquete `sf`, la colección `tidyverse` y los paquetes `spdep`, `lmtest`, `tmap` y `RColorBrewer`
```{r}
library(sf)
library(tidyverse)
library(spdep)
library(lmtest)
library(tmap)
library(RColorBrewer)
```
## Datos y unión

* Carga el conjunto de datos de "ENHOGAR 2017" (`.csv`), asignándolo al objeto `en17`. Nota que "ENHOGAR 2017" es una encuesta y, por lo tanto, recoge resultados referidos a una muestra, cuyo tamaño por provincia está recogido en la columna `muestra`. Carga también la capa geométrica (`.gpkg`) asignándola al objeto `prov`, y únelo a `en17`. Ambas fuentes se encuentran en la carpeta `data`. Recuerda el problema de la inconsistencia en el código entre ambas fuentes. Verifica consistencia luego de corregir el código (usa la práctica anterior como apoyo). Finalmente, une ambas fuentes, `prov` y `en17`.

```{r}
en17 <- read.csv('material-de-apoyo-master/data/enhogar_2017.csv', check.names = F)
prov <- st_read(dsn = 'material-de-apoyo-master/data/divisionRD.gpkg', layer = 'PROVcenso2010')
en17 <- en17 %>% mutate(ENLACE = ifelse(nchar(Código)==3, paste0('0', Código),Código))
match(en17$ENLACE, prov$ENLACE)
proven17 <- prov %>% inner_join(en17, by = 'ENLACE')
```
* Se Imprime en pantalla el `sf` resultante y genera un mapa que muestre tu pregunta para todo el país.
```{r}

proven17 %>%
  dplyr::select(contains('A que cree que se debe la delincuencia en el pais ¿A la falta de alternativas sanas (clubes, cine, teatro, etc.) para el entretenimiento?: Si')) %>%
  plot(breaks = 'jenks')
```

## Conversión a `sp`

* Conviertiendo el objeto `proven17` a `SpatialPolygonsDataFrame` asignándolo a `proven17.sp`, mediante la función `as_Spatial`. Este paso es necesario para crear los objetos de vecindad. Verificamos que los nombres de columnas `proven17.sp` aparecen deformados (espacios sustituidos por puntos), y lo corrígimos rescatando los nombres del objeto original `proven17`.

```{r}
proven17.sp <- as_Spatial(proven17)
colnames(proven17.sp@data)[1:20]
colnames(proven17.sp@data) <- proven17 %>% st_drop_geometry() %>% colnames

```

* Asignamos nombres de filas al objeto `proven17.sp` a partir de la columna `TOPONIMIA`.

```{r}
row.names(proven17.sp) <- as.character(proven17.sp$TOPONIMIA)
```
## Vecindad por contigüidad

* A partir de `proven17.sp`, creamos un objeto de vecindad por contigüidad, asignándolo a `proven17.nb`, usando criterio `queen`. Se Imprime en pantalla el resumen de dicho objeto de vecindad.

```{r}
proven17.nb <- poly2nb(proven17.sp, queen=TRUE)
summary(proven17.nb)
```
* Evalúamos la cardinalidad, es decir, cuántos vecinos tiene cada geometría/elemento (que en este caso son provincias).

```{r}
card(proven17.nb)
```
* Imprimimos en pantalla la relación de vecinos de cada geometría.
```{r}
sapply(proven17.nb, function(x) x)
```

* Construimos un mapa de los vínculos de vecindad (grafo). Recuerdar que primero debemos de generar un mapa de las provincias, y luego se superpondrán los vínculos.


```{r}
plot(proven17.sp, border="grey", lwd=0.5)
plot(proven17.nb, coordinates(proven17.sp), add=T)
```
* Evalúamos si el objeto de vecindad es simétrico.

```{r}
is.symmetric.nb(proven17.nb)
```
## Vecindad por número de vecinos

* A partir de `proven17.sp`, crearemos un objeto de vecindad por número de vecinos, en el que cada geometría tenga sólo un vecino, asignándolo a `proven17.nb.k1`. Imprime en pantalla el resumen de dicho objeto de vecindad. Recuerda crear un objeto de coordenadas de centroides, que en este ejercicio se sugiere con el nombre `coords`, y otro de identidades de cada geometría, para el cual se sugiere el nombre `ident`; ambos los usarás dentro de la función `knn2nb`
El resumen del objeto `proven17.nb.k1` debería mostrar 32 vínculos, el mismo número de regiones de `proven17.sp`, dado que cada región tiene un único vínculo.

```{r}
coords <- coordinates(proven17.sp)
ident <- row.names(proven17.sp)
proven17.nb.k1 <- knn2nb(knearneigh(coords, k = 1), row.names = ident)
summary(proven17.nb.k1)
```
* Evalúamos la cardinalidad, es decir, cuántos vecinos tiene cada geometría/elemento (que en este caso son provincias). Dado que se especificó anteriormente que sólo hubiese un único vecino, el vector debería ser una repetición de `1`.

```{r}
card(proven17.nb)
```
* Imprime en pantalla la relación de vecinos de cada geometría.

```{r}
sapply(proven17.nb, function(x) x)
```
* Hacemos un mapa de los vínculos de vecindad (grafo). Recuerdar que primero debemos generar un mapa de las provincias (primera corrida de la función `plot`), y luego le superpondrás el mapa de los vínculos (segunda corrida de `plot`, con argumento `add=T`)

```{r}
plot(proven17.sp, border="grey", lwd=0.5)
plot(proven17.nb.k1, coordinates(proven17.sp), add=T)
```

* Evalúamos si el objeto de vecindad es simétrico 

```{r}
is.symmetric.nb(proven17.nb)
```
* Exploramos las distancias entre centroides de las geometrías a partir del objeto `proven17.nb.k1`. Para ello, creamos un objeto denominado `dist` donde se almacenan las distancias a partir de aplicar la función `nbdists` (recuerda que dentro de ésta debemos colocar el objeto `coords`). Esto Imprime en pantalla un resumen estadístico, y genera un histograma y un boxplot.

```{r}
dist <- unlist(nbdists(proven17.nb.k1, coords))
summary(dist)
hist(dist)
boxplot(dist)
```
* Generaremos un objeto con la distancia mínima (objeto `distmin` usando la función `min`) y otro con la máxima (objeto `distmax` usando la función `max`). Luego, se determina qué posición ocupa en el vector `dist` ocupan los valores de `distmin` de `distmax`, y asígnalas a los objetos `indicemin` y `indicemax`, respectivamente. Posteriormente, utiliza dichas posiciones (`indicemin` y `indicemax`) dentro del índice de `ident` para determinar cuál o cuáles provincias se encuentran a la menor y a la mayor distancia en el conjunto del país.

```{r}
(distmin <- min(dist)) 
(distmax <- max(dist))
indicemin <- which(dist==distmin)
ident[indicemin]
indicemax <- which(dist==distmax)
ident[indicemax]
```
* Ordenaremos los nombres de provincias de menor a mayor distancia de separación con su vecino más próximo.

```{r}
ident[order(dist)]
```
## Ponderadores espaciales

* Generamos dos objetos de pesos espaciales a partir del objeto de vecindad por contigüidad; uno de ellos estandarizado por filas (asígnalo a `proven17.w.W`) y otro binario (asígnalo a `proven17.w.B`)

```{r}
proven17.w.W <- nb2listw(proven17.nb)
proven17.w.W
proven17.w.B <- nb2listw(proven17.nb, style = 'B')
proven17.w.B
```
## Autocorrelación espacial de nuestra variable

Exploramos la autocorrelación espacial de nuestra variable utilizando el *I* de Moran global y el local.

* Usando `tidyverse`, generamos una columna de porcentaje de personas que respondió a nuestra pregunta respecto del tamaño de la muestra a nivel provincial (columna `muestra`). le Pondremos por nombre `mivariable_pct`. Generamos una transformada a partir de la anterior, y le pondremos el nombre `mivariable_pct_log`. El objeto `sf` resultante se asígnara a `proven17_mivar_sf`


```{r}
mivariable <- 'A qué cree que se debe la delincuencia en el país: ¿A la falta de alternativas sanas (clubes, cine, teatro, etc.) para el entretenimiento?: Si'
proven17_mivar <- proven17 %>%
  st_centroid() %>% 
  select(ENLACE, mivariable=contains(mivariable), muestra) %>% 
  mutate('mivariable_pct' = mivariable/muestra*100,
         'mivariable_pct_log' = log1p(mivariable/muestra*100),
         x=unlist(map(geom,1)),
         y=unlist(map(geom,2))) %>%
  select(-muestra) %>% 
  st_drop_geometry()
proven17_mivar_sf <- proven17 %>%
  inner_join(proven17_mivar, by = 'ENLACE') %>% 
  dplyr::select(muestra, contains('mivariable'), x, y, ENLACE, TOPONIMIA)
```

* Hacemos un mapa que muestre la variable, tanto en su versión original como transformada.

```{r pctfueramaps}
p1 <- tm_shape(proven17_mivar_sf) +
  tm_fill(col = "mivariable_pct", style = 'jenks',
          palette = brewer.pal(9, name = 'Reds'), title = mivariable) +
  tm_borders(lwd = 0.5)
p2 <- tm_shape(proven17_mivar_sf) +
  tm_fill(col = "mivariable_pct_log", style = 'jenks',
          palette = brewer.pal(9, name = 'Reds'), midpoint = NA, title = mivariable) +
  tm_borders(lwd = 0.5) 
tmap_arrange(p1, p2)
```
* Compruebamos el supuesto de normalidad de nuestra variable, tanto en su versión original como transformada, mediante el gráfico cuantilar normal y la prueba de *Shapiro-Wilk*.

>Tip: Como argumento de las funciones a continuación, usa la forma vectorial de tu variable original y transformada; e.g.proven17_mivar_sf$mivariable_pct_log

>Tip: Si los puntos del gráfico cuantilar normal siguen una línea recta, y la prueba de Shapiro-Wilk resulta no significativa (es decir, el valor de *p* mayor que 0.05), entonces se asume como válido el supuesto de normalidad.

```{r}
qqnorm(proven17_mivar_sf$mivariable_pct) #Versión original de la variable
shapiro.test(proven17_mivar_sf$mivariable_pct) #Versión original de la variable
qqnorm(proven17_mivar_sf$mivariable_pct_log) #Versión transformada de la variable (log)
shapiro.test(proven17_mivar_sf$mivariable_pct_log) #Versión transformada de la variable (log)
```
# Resultados
* Interpreta el resultado de la comprobación anterior aquí:

#1-para la variable original=prueba de Shapiro-Wilk resulta  significativa (es decir, el valor de *p* menor que 0.05), entonces se asume como no válido el supuesto de normalidad.
#2-para la variable modificada=prueba de Shaprio-Wilk resulta no significativa (es decir, el valor de *p*mayor que 0.05), entonces se asume como  válido el supuesto de normalidad.

* Comprobamos el supuesto de homocedasticidad de tu variable respecto de `x` e `y`, tanto en su versión original como en la transformada, mediante la prueba de *Breusch-Pagan*.

>Tip: Si el valor de *p* es menor que 0.05 (nivel de significancia convencional, aunque arbitrario), existe evidencia para rechazar la hipótesis de homocedasticidad.

```{r}
proven17_mivar_sf %>% lm(mivariable_pct ~ x, .) %>% bptest()
proven17_mivar_sf %>% lm(mivariable_pct ~ y, .) %>% bptest()
proven17_mivar_sf %>% lm(mivariable_pct_log ~ x, .) %>% bptest()
proven17_mivar_sf %>% lm(mivariable_pct_log ~ y, .) %>% bptest()
```
* Interpreta el resultado de la comprobación anterior aquí:
#1-para la variable original=el valor de *p* es mayor que 0.05 (nivel de significancia no convencional, aunque arbitrario), no existe evidencia para rechazar la hipótesis de homocedasticidad.
#2-para la variable modificada=el valor de *p* es mayor que 0.05 (nivel de significancia no convencional, aunque arbitrario), no existe evidencia para rechazar la hipótesis de homocedasticidad.

En la eventualidad de que el supuesto normalidad y el de homocedasticidad no se cumplan, continúa con el procedimiento de estimar la autocorrelación la versión original o la transformada de tu variable, según elijas, aun cuando los resultados del análisis de autocorrelación espacial podrían no ser fiables.

# Autocorrelación espacial global

* Comprobamos primero que existe consistencia en la secuencia de los nombres del objeto de vecindad y el *sf*.

```{r}
match(attr(proven17.w.W$neighbours, "region.id"), proven17_mivar_sf$TOPONIMIA)==1:32
```

* Aplicamos la prueba de autocorrelación espacial global para el *I* de Moran, usando los pesos estandarizados por filas como los binarios.

```{r}
(gmoranw <- moran.test(x = proven17_mivar_sf$mivariable_pct_log, listw = proven17.w.W))
(gmoranb <- moran.test(x = proven17_mivar_sf$mivariable_pct_log, listw = proven17.w.B))
```
* Interpreta el resultado de la comprobación anterior aquí:

#1-para los pesos estandarizados=el valor de *p* obtenido fue menor de 0.05, hay evidencia preliminar para rechazar la hipótesis nula de "no hay autocorrelación espacial", y por lo tanto concluir que "si hay autocorrelación espacial"

#2-para los pesos binarios=el valor de *p* obtenido fue menor de 0.05, hay evidencia preliminar para rechazar la hipótesis nula de "no hay autocorrelación espacial", y por lo tanto concluir que "si hay autocorrelación espacial"

>Tip: Si el valor de *p* obtenido fue menor de 0.05, hay evidencia preliminar para rechazar la hipótesis nula de "no hay autocorrelación espacial", y por lo tanto concluir que "sí hay autocorrelación espacial". En cualquier caso, es necesario que continúes evaluando la autocorrelación a nivel local en el siguiente paso, con independencia del resultado obtenido en la prueba global.

* Evalúamos la autocorrelación espacial local. Realizamos el diagrama de dispersión de Moran (*Moran scatterplot*), mediante la función `moran.plot`. Posteriormente, cargamos el script `lisaclusters.R`, y ejecutamos la función `lisamap` a nuestros datos para generar el mapa LISA. En la función `lisamap`, debemos introducir los siguientes argumentos: `objesp`, que es el objeto espacial denominado `proven17_mivar_sf`; `pesos`, que es el objeto de pesos, que será `proven17.w.W`.

```{r}
moran.plot(x = proven17_mivar_sf$mivariable_pct_log, listw = proven17.w.W)
source('lisaclusters.R')
lisamap(objesp = proven17_mivar_sf,
        var = 'mivariable_pct_log',
        pesos = proven17.w.W,
        tituloleyenda = 'Significancia\n("x-y", léase\ncomo "x"\nrodeado de "y"',
        leyenda = T,
        anchuratitulo = 1000,
        tamanotitulo = 16,
        fuentedatos = 'ENHOGAR 2017',
        titulomapa = paste0('Clusters LISA de respuestas a la pregunta:\n"', mivariable, '"'))
```
* Interpreta el resultado anterior aquí: 

#1-HAY un patron de relleno rojo y azul significa que existe autocorrelación local
#2-HAY Un patrón de varias provincias coloreadas de rojo se atribuye a "efecto de contagio" importante. esto siginifica que hay autocorrelación espacial local
#3-el las demas provincias se observa el gris siginifica que no hay autocorrelación espacial local

>Tip: Si existe relleno rojo o azul, o ambos, significa que existe autocorrelación local. El relleno rojo (*hotspots*) significa que los valores altos del grupo coloreado son parecidos entre sí. Un patrón de varias provincias coloreadas de rojo se atribuye a "efecto de contagio" importante. Si sólo una provincia aparece en rojo, significa que las provincias de su entorno tienen valores parecidos, pero éstas últimas a su vez no tienen valores significativamente parecidos con su entorno ulterior (no se produce "contagio"). El relleno azul (*coldspots*) se interpreta de la misma manera que en el caso anterior, pero con los valores en este caso son bajos. Si sólo aparecen rellenos grises, significa que no hay autocorrelación local, y las provincias entonces presentan un patrón aleatorio de la variable analizada.

>A modo de verificación, los mapas LISA de todas las variables asignadas se transcriben a continuación. Nota que, de las variables asignadas, algunas no presentan autocorrelación espacial local.


# Discusión o Conclusiones
Atraves de este procedimientos pudimos verificar que la delicuencia en el pais se puede contagiar a varias provincias vecinas y que influye mucho  en el crecimiento de la delincuencia la faltas de alternativas sanas como son clubes, cine , teatro etc. para el entretenimiento. atraves de la ejecucion del codigo pudimos darmos cuenta que el mismo tenia una distribucion normal en la variable modificada. mediante la prueva de Shapiro-Wilk, Breusch-Pagan y I de moran pudimos establecer los criterios para cada prueba descritos mas arriba. tambien pudimos demostrar que existe autocorrelacion espacial local y efecto de contagio importante.



\ldots

# Información de soporte
Codigos, procedimientos de la clase de Vecindad y autocorrelacion espacial del profesor Jose Ramon Martinez Batlle.

\ldots

# *Script* reproducible

\ldots

# Referencias
Material de apoyo, suministrado por el profesor Jose Ramon Martinez Batlle.
Capa de division de Provincia de La ONE. (Oficina Nacional de Estadisticas)
Encuesta En hogar de la ONE.(Oficina Nacional de Estadisticas)
Capa de ProvCenso2010 de la ONE.(Oficina Nacional de Estadisticas)
