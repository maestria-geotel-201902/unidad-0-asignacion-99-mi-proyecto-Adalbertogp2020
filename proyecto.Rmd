---
title: "| Autocorrelacion de la Delinciencia por Provincia,\n| Geoestadistica de la
  \ Precipitacion del año 1998 \n| y Modelización de Datos Espaciales.\n"
author:
- affiliation: Estudiante, Universidad Autónoma de Santo Domingo (UASD)
  name: Adalberto Guerrero Portorreal.
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    latex_engine: pdflatex
    number_sections: yes
    template: svm-latex-ms.tex
bibliography: bibliography.bib
editor_options:
  chunk_output_type: console
fontfamily: mathpazo
fontsize: 11pt
geometry: margin=1in
header-includes: \usepackage{pdflscape} \newcommand{\blandscape}{\begin{landscape}}
  \newcommand{\elandscape}{\end{landscape}}
keywords: delincuencia, entretenimiento
csl: apa.csl
abstract: Mi resumen
---
 Profesor: José Ramón Martínez Batlle.
 
 Este trabajo es realizado como presentación final de la asignatura Análisis Espacial de la Maestría en Teledetección y Ciencias   Geográficas, la cual fue impartida en la Escuela de Ciencias Geográficas de la UASD.
 
 
# PARTE I

## Autocorrelacion de la Delincuencia por Provincia

### Introducción.

En este tema de Vecindad y Autocorrealcion, analizaremos que tanto se correlación las personas de los municipios con su entorno con sus vecinos más cercanos y que tanto incide nuestra variable dependiente analizada cuanto hay contigüidad.
podremos mirar que pasa con la delincuencia en el país a la falta de alternativas sana para el entretenimiento como esto puede contagia a los municipios vecinos hasta la posibilidad de convertirse en una epidemia local. a nivel espacial. evaluaremos si hay efectos de contagio o si son casos que solo se concentran en un municipio y no se extienden a los demás.
'A qué cree que se debe la delincuencia en el país ¿A la falta de alternativas sanas (clubes, cine, teatro, etc.) para el entretenimiento?: Si'


### Metodología

El desarrollo de este proyecto se hara mediante el lenguaje de programacion R Y la plataforma Rstudio, cuyo usuario y permiso sera habilitado por el profesor.
Usaremos una metodologia explicada por el profesor en clase, donde en primer lugar Se cargan los paquetes o librerías de R las cuales serviran de soporte para realizar disversas operaciones a medida que se vaya desarrollando el Scribt; Cargamos los datos de la escuesta Nacional de Hogares de Propósitos Múltiples de 2017 (ENHOGAR-2017) donde navegaremos por los distintos campos de nuestro interes Y la capa de provincias dominicanas, haremos pruebas estadísticas verificando la normalidad y si hay o no relación con otras variables.
Elegimos una pregunta de investigacion, sobre la cual sera realizado el análisis de autocorrelación.
Tambien  Realizaremos el análisis exploratorio de datos espaciales, funciones de homogeneidad espacial, 
ademas de el análisis de autocorrelación espacial global e interpretacion de resultados.
Formulamos Discusión o Conclusiones y al final un scrib reproducible. 


### *Script* reproducible

### Paquetes


* Carga el paquete `sf`, la colección `tidyverse` y los paquetes `spdep`, `lmtest`, `tmap` y `RColorBrewer`
```{r}
library(sf)
library(tidyverse)
library(spdep)
library(lmtest)
library(tmap)
library(RColorBrewer)
```

### Datos 

* Carga el conjunto de datos de "ENHOGAR 2017" (`.csv`), asignándolo al objeto `en17`. Nota La Encuesta Nacional de Hogares de Propósitos Múltiples (ENHOGAR) es un mecanismo de respuesta oportuna a las demandas de información urgente para la agenda nacional que de forma recurrente requiere de información oportuna y confiable, tanto desde la perspectiva de las políticas públicas, como para la toma de decisiones fuera del ámbito oficial. Este proyecto surgió a partir de la iniciativa que significó el Programa MECOVI (Programa para el Mejoramiento de las encuestas y la Medición de las Condiciones de Vida en América Latina y El Caribe).


```{r}
en17 <- read.csv('material-de-apoyo-master/data/enhogar_2017.csv', check.names = F)
prov <- st_read(dsn = 'material-de-apoyo-master/data/divisionRD.gpkg', layer = 'PROVcenso2010')
en17 <- en17 %>% mutate(ENLACE = ifelse(nchar(Código)==3, paste0('0', Código),Código))
match(en17$ENLACE, prov$ENLACE)
proven17 <- prov %>% inner_join(en17, by = 'ENLACE')
```


* Haremos la imprecion en pantalla del  `sf` resultante.


```{r}
proven17 %>%
  dplyr::select(contains('A que cree que se debe la delincuencia en el pais ¿A la falta de alternativas sanas (clubes, cine, teatro, etc.) para el entretenimiento?: Si')) %>%
  plot(breaks = 'jenks')
```


### Conversión a `sp`


* En este paso convertimos el objeto `proven17` a `SpatialPolygonsDataFrame` asignándolo a `proven17.sp`, mediante la función `as_Spatial`. 


```{r}
proven17.sp <- as_Spatial(proven17)
colnames(proven17.sp@data)[1:20]
colnames(proven17.sp@data) <- proven17 %>% st_drop_geometry() %>% colnames
```


* Asignamos nombres de filas al objeto `proven17.sp` a partir de la columna `TOPONIMIA`.


```{r}
row.names(proven17.sp) <- as.character(proven17.sp$TOPONIMIA)
```


### Vecindad por Contigüidad


* A partir de `proven17.sp`, creamos un objeto de vecindad por contigüidad, asignándolo a `proven17.nb`, luego. Se Imprime en pantalla el resumen de dicho objeto de vecindad.


```{r}
proven17.nb <- poly2nb(proven17.sp, queen=TRUE)
summary(proven17.nb)
```
* Evalúanos la cardinalidad, es decir, cuántos vecinos tiene cada geometría/elemento (que en este caso son provincias).


```{r}
card(proven17.nb)
```


* Imprimimos en pantalla la relación de vecinos de cada geometría.


```{r}
sapply(proven17.nb, function(x) x)
```


* Construimos un mapa de los vínculos de vecindad (grafo). al igual que un mapa de las provincias.


```{r}
plot(proven17.sp, border="grey", lwd=0.5)
plot(proven17.nb, coordinates(proven17.sp), add=T)
```


* Evalúamos si el objeto de vecindad es simétrico.



```{r}
is.symmetric.nb(proven17.nb)
```


### Vecindad por Número de Vecinos


* Después de `proven17.sp`, crearemos un objeto de vecindad por número de vecinos, en el que cada geometría tenga sólo un vecino, asignándolo a `proven17.nb.k1`. Imprime en pantalla el resumen de dicho objeto de vecindad. Recuerda crear un objeto de coordenadas de centroides, que en este ejercicio se sugiere con el nombre `coords`, y otro de identidades de cada geometría, para el cual se sugiere el nombre `ident`; en ambos los usamos dentro de la función `knn2nb`
El resumen del objeto `proven17.nb.k1 mostrara 32 vínculos, el mismo número de regiones de `proven17.sp`


```{r}
coords <- coordinates(proven17.sp)
ident <- row.names(proven17.sp)
proven17.nb.k1 <- knn2nb(knearneigh(coords, k = 1), row.names = ident)
summary(proven17.nb.k1)
```


* Evalúanos la cardinalidad, me refiero a cuántos vecinos tiene cada geometría/elemento. Dado que se especificó anteriormente que sólo hubiese un único vecino.

```{r}
card(proven17.nb)
```


* Se imprime en pantalla la relación de vecinos de cada geometría.


```{r}
sapply(proven17.nb, function(x) x)
```


* Construimos un mapa de los vínculos de vecindad (grafo). Debemos generar primero un mapa de las provincias (primera corrida de la función `plot`), y luego le superpondremos el mapa de los vínculos (segunda corrida de `plot`, con el argumento `add=T`)


```{r}
plot(proven17.sp, border="grey", lwd=0.5)
plot(proven17.nb.k1, coordinates(proven17.sp), add=T)
```


* Evalúando si el objeto de vecindad es simétrico 


```{r}
is.symmetric.nb(proven17.nb)
```


* Estudiamos las distancias entre centro de las geometrías a partir del objeto `proven17.nb.k1`. Para esto, creamos un objeto denominado `dist` donde se almacenan las distancias a partir de aplicar la función `nbdists` (recordar colocar el objeto `coords`). Esta función Imprime en pantalla un resumen estadístico, y genera un histograma y un boxplot.


```{r}
dist <- unlist(nbdists(proven17.nb.k1, coords))
summary(dist)
hist(dist)
boxplot(dist)
```


* Generaremos un objeto con la distancia mínima (objeto `distmin` usando la función `min`) y otro con la máxima (objeto `distmax` usando la función `max`). y las asígnamos a los objetos `indicemin` y `indicemax`. Luego, utiliza dichas posiciones (`indicemin` y `indicemax`) dentro del índice de `ident` para determinar cuál o cuáles provincias se encuentran a la menor y a la mayor distancia en el conjunto del país.



```{r}
(distmin <- min(dist)) 
(distmax <- max(dist))
indicemin <- which(dist==distmin)
ident[indicemin]
indicemax <- which(dist==distmax)
ident[indicemax]
```


* Ordenaremos los nombres de provincias de menor a mayor distancia de separación con su vecino más próximo.


```{r}
ident[order(dist)]
```


### Ponderadores Espaciales de Pesos, Estandarizados y Binarios.


* Generamos dos objetos de pesos espaciales a partir del objeto de vecindad por contigüidad; uno de ellos estandarizado por filas (asígnalo a `proven17.w.W`) y otro binario, luego lo asignamos a `proven17.w.B`)


```{r}
proven17.w.W <- nb2listw(proven17.nb)
proven17.w.W
proven17.w.B <- nb2listw(proven17.nb, style = 'B')
proven17.w.B
```


### Autocorrelación Espacial de Nuestra Variable


Exploramos la autocorrelación espacial de nuestra variable utilizando el *I* de Moran global y el local.



```{r}
mivariable <- 'A qué cree que se debe la delincuencia en el país: ¿A la falta de alternativas sanas (clubes, cine, teatro, etc.) para el entretenimiento?: Si'
proven17_mivar <- proven17 %>%
  st_centroid() %>% 
  select(ENLACE, mivariable=contains(mivariable), muestra) %>% 
  mutate('mivariable_pct' = mivariable/muestra*100,
         'mivariable_pct_log' = log1p(mivariable/muestra*100),
         x=unlist(map(geom,1)),
         y=unlist(map(geom,2))) %>%
  select(-muestra) %>% 
  st_drop_geometry()
proven17_mivar_sf <- proven17 %>%
  inner_join(proven17_mivar, by = 'ENLACE') %>% 
  dplyr::select(muestra, contains('mivariable'), x, y, ENLACE, TOPONIMIA)
```



* Hacemos un mapa que muestre la variable, tanto en su versión original como transformada.


```{r pctfueramaps}
p1 <- tm_shape(proven17_mivar_sf) +
  tm_fill(col = "mivariable_pct", style = 'jenks',
          palette = brewer.pal(9, name = 'Reds'), title = mivariable) +
  tm_borders(lwd = 0.5)
p2 <- tm_shape(proven17_mivar_sf) +
  tm_fill(col = "mivariable_pct_log", style = 'jenks',
          palette = brewer.pal(9, name = 'Reds'), midpoint = NA, title = mivariable) +
  tm_borders(lwd = 0.5) 
tmap_arrange(p1, p2)
```


* Comprobamos el supuesto de normalidad de nuestra variable, tanto en su versión original como transformada, mediante el gráfico cuantilar normal y la prueba de *Shapiro-Wilk*.


```{r}
qqnorm(proven17_mivar_sf$mivariable_pct) #Versión original de la variable
shapiro.test(proven17_mivar_sf$mivariable_pct) #Versión original de la variable
qqnorm(proven17_mivar_sf$mivariable_pct_log) #Versión transformada de la variable (log)
shapiro.test(proven17_mivar_sf$mivariable_pct_log) #Versión transformada de la variable (log)
```


### Resultados


* Interpreta el resultado de la comprobación anterior aquí:

1-Para la variable original=prueba de Shapiro-Wilk resulta significativa (es decir, el valor de *p* menor que 0.05), entonces se asume como no válido el supuesto de normalidad.


2-Para la variable modificada=prueba de Shaprio-Wilk resulta no significativa (es decir, el valor de *p*mayor que 0.05), entonces se asume como válido el supuesto de normalidad.


* Comprobamos el supuesto de homocedasticidad de tu variable respecto de `x` e `y`, tanto en su versión original como en la transformada, mediante la prueba de *Breusch-Pagan*.



```{r}
proven17_mivar_sf %>% lm(mivariable_pct ~ x, .) %>% bptest()
proven17_mivar_sf %>% lm(mivariable_pct ~ y, .) %>% bptest()
proven17_mivar_sf %>% lm(mivariable_pct_log ~ x, .) %>% bptest()
proven17_mivar_sf %>% lm(mivariable_pct_log ~ y, .) %>% bptest()
```


* Interpreta el resultado de la comprobación anterior aquí:

1-Para la variable original=el valor de *p* es mayor que 0.05 (nivel de significancia no convencional, aunque arbitrario), no existe evidencia para rechazar la hipótesis de homocedasticidad.

2-Para la variable modificada=el valor de *p* es mayor que 0.05 (nivel de significancia no convencional, aunque arbitrario), no existe evidencia para rechazar la hipótesis de homocedasticidad.

En la eventualidad de que el supuesto normalidad y el de homocedasticidad no se cumplan, continúa con el procedimiento de estimar la autocorrelación la versión original o la transformada de tu variable, según elijas, aun cuando los resultados del análisis de autocorrelación espacial podrían no ser fiables.


### Autocorrelación Espacial Global


* Comprobacion de la consistencia en la secuencia de los nombres del objeto de vecindad y el *sf*.


```{r}
match(attr(proven17.w.W$neighbours, "region.id"), proven17_mivar_sf$TOPONIMIA)==1:32
```


* Aplicamos la prueba de autocorrelación espacial global para el *I* de Moran, usando los pesos estandarizados por filas como los binarios.


```{r}
(gmoranw <- moran.test(x = proven17_mivar_sf$mivariable_pct_log, listw = proven17.w.W))
(gmoranb <- moran.test(x = proven17_mivar_sf$mivariable_pct_log, listw = proven17.w.B))
```


* Interpreta el resultado de la comprobación anterior aquí:

1-Para los pesos estandarizados=el valor de *p* obtenido fue menor de 0.05, hay evidencia preliminar para rechazar la hipótesis nula de "no hay autocorrelación espacial", y por lo tanto concluir que "si hay autocorrelación espacial"

2-Para los pesos binarios=el valor de *p* obtenido fue menor de 0.05, hay evidencia preliminar para rechazar la hipótesis nula de "no hay autocorrelación espacial", y por lo tanto concluir que "si hay autocorrelación espacial"


### Evalúamos la autocorrelación espacial local. 


```{r}
moran.plot(x = proven17_mivar_sf$mivariable_pct_log, listw = proven17.w.W)
source('lisaclusters.R')
lisamap(objesp = proven17_mivar_sf,
        var = 'mivariable_pct_log',
        pesos = proven17.w.W,
        tituloleyenda = 'Significancia\n("x-y", léase\ncomo "x"\nrodeado de "y"',
        leyenda = T,
        anchuratitulo = 1000,
        tamanotitulo = 16,
        fuentedatos = 'ENHOGAR 2017',
        titulomapa = paste0('Clusters LISA de respuestas a la pregunta:\n"', mivariable, '"'))
```


* Interpretación de los resultados obtenidos atraves del moran plot: 

1-Hay un patrón de relleno rojo y azul significa que existe autocorrelación local

2-Hay Un patrón de varias provincias coloreadas de rojo se atribuye a "efecto de contagio" importante. esto significa que hay autocorrelación espacial local

3-En las demás provincias se observa el gris significa que no hay autocorrelación espacial local


### Discusión o Conclusiones


Atraves de este procedimiento pudimos verificar que la delincuencia en el país se puede contagiar a varias provincias vecinas y que influye mucho en el crecimiento de la delincuencia la falta de alternativas sanas como son clubes, cine, teatro etc. para el entretenimiento. atraves de la ejecución del código pudimos darnos cuenta que el mismo tenía una distribución normal en la variable modificada. mediante la prueba de Shapiro-Wilk, Breusch-Pagan y I de moran pudimos establecer los criterios para cada prueba descritos más arriba. también pudimos demostrar que existe autocorrelacion espacial local y efecto de contagio importante



\ldots

### Información de Soporte


Códigos, procedimientos de la clase de Vecindad y autocorrelacion espacial del profesor José Ramón Martínez Batlle.

\ldots

### Referencias Bibliograficas.

https://www.one.gob.do/encuestas/enhogar
BIVAND-PEBESMA-GOMEZ-RUBIO-analisis-espacial-aplicado-con-R.pdf
ISAAKS-SRIVASTAVA-una-introduccion-a-la-geoestadistica-aplicada.pdf
MAS-analisis-espacial-con-R-usa-R-como-un-sistema-informacion-geografia-.pdf
Capa de división de Provincia de La ONE. (Oficina Nacional de Estadísticas)
Encuesta En hogar de la ONE. (Oficina Nacional de Estadísticas)
Capa de ProvCenso2010 de la ONE. (Oficina Nacional de Estadísticas)



# PARTE II

## Datos Puntuales Superficie Continua y Creación de Isolíneas en R (Mapas de Precipitación).

### Introducción.

El presente proyecto se trata de generar una superficie continua y atraves de ella crear un mapa de isoyetas, para esto utilizaremos la capa de provincias de la OFICINA NACIONAL DE ESTADISTICA (ONE)  y los datos de lluvia de la OFICINA NACIONAL DE METEOROLOGIA (ONAMET) los datos de lluvia corresponden al año 1998, año en el cual fuimos golpeados por un fenómeno meteorológico muy fuerte que causo muchos daños al país, causo inundaciones en casi en todo el territorio nacional así como grandes áreas de bosques y cultivos devastadas, debido a sus fuertes vientos. el nombre de este fenómeno es el Ciclón GEORGE. en el país ocurrieron muchas lluvias durante casi todo el año 1998.es por esta razón nuestro interés de realizar el análisis para este tiempo.

### Metodología

Para realizar este proyecto primero debemos generar una superficie continúa usando los datos de lluvia para el año 1998 y la capa de provincia.
Cargamos la librería (library(gstat) cuyos datos serán usados más adelante y los demás paquetes no son necesarios puesto que ya están cargados en la parte I.
Combinando las diferentes líneas de códigos aprendidas durante el desarrollo de esta materia. 
Crearemos una serie de instrumentos estadístico como son el variograma muestral, variograma modelo, Interpolación por Kriging Ordinario.
Al final para general el mapa de isoyetas bastara con ejecutar el paquete contour data. disponible para R. y realizar algunos ajustes para la presentación del mapa.
.



\ldots

### *Script* reproducible

### Paquetes

* Como ya cargamos los paquetes `sf`, la colección `tidyverse` y los paquetes `spdep`, `lmtest`, `tmap` y `RColorBrewer` en la primera parte de este proyecto, solo queda cargar el paquete `gstat`.

```{r}
library(gstat)
```

### Cargar Datos

```{r}
rutapre <- 'material-de-apoyo-master/data/onamet_prec_anual_sf.gpkg'
rutadiv <- 'material-de-apoyo-master/data/divisionRD.gpkg'
pre <- st_read(rutapre)
prov <- st_read(rutadiv, layer = 'PROVCenso2010')
```

### Transformar Datos.

```{r}
st_crs(pre)
crsdestino <- 32619
preutm <- pre %>% st_transform(crs = crsdestino)
preutm
```

### EDA Básico

   ahora vamos a construir los datos para el año 1998:
   
```{r}
nrow(preutm)
summary(preutm$a1998)
hist(preutm$a1998)
hist(log(preutm$a1998))
shapiro.test(preutm$a1998)
shapiro.test(log(pre$a1998))
```

Según el histograma los datos siguen distribución normal para la variable modificada, Igualmente, de los 25 pluviómetros  que teníamos en el país para el año 1998  hay dos con  datos perdidos (NA). Eliminemos dichos datos, y crearemos solo los objetos de 1998 que tengan datos:


```{r}
pre1998 <- na.omit(preutm[,c('Estación', 'a1998')])
pre1998$a1998log <- log(pre1998$a1998)
pre1998
```

### Visualizamos los Observatorios, ya Depurados Según la Precipitación del año 1998:


```{r}
library(ggplot2)
ggplot() +
  geom_sf(data = prov, fill = 'white') +
  geom_sf(data = pre1998, aes(col = a1998log), size = 6) +
  scale_colour_gradient(low="#deebf7", high="#3182bd") +
  geom_sf_text(data = prov, aes(label=TOPONIMIA), check_overlap = T, size = 2) +
  geom_sf_text(data = pre1998, aes(label=Estación), check_overlap = T, size = 1.5) +
  theme_bw()
```

### Variograma Muestral
   
   Crearemos el variograma muestral para la variable modificada de la precipitación    o sea la parte logaritmica.
   
```{r}
f98 <- variogram(a1998log~1, pre1998)
f98
plot(f98, plot.numbers = T)
```

### Variograma Modelo.

Después de construir el variograma muestral, vamos a construir un variograma modelo para esto utilizaremos la función Krige para interpolar los 
      
      
```{r}
f98_m <- fit.variogram(f98, vgm(model = "Sph", range = 50000))
f98_m
plot(f98, f98_m, plot.numbers = T)
f98_m2 <- fit.variogram(f98, vgm(model = "Exp", range = 50000))
f98_m2
plot(f98, f98_m2, plot.numbers = T)
f98_m3 <- fit.variogram(f98, vgm(model = "Gau", range = 50000))
f98_m3
plot(f98, f98_m3, plot.numbers = T)
attr(f98_m, 'SSErr')
attr(f98_m2, 'SSErr') 
attr(f98_m3, 'SSErr')
```
 
### Interpolación por Kriging Ordinario

Para esta interpolación crearemos una cuadrícula con las precipitaciones.     
una cuadrícula apropiada para RD, sería una de baja resolución, por ejemplo 1x1km:  

       
    
```{r}
library(stars)
grd <- st_bbox(prov) %>%
  st_as_stars(dx = 1000) %>% 
  st_set_crs(crsdestino) %>%
  st_crop(prov)
grd
plot(grd)
```
    
Sobre esta superficie continua la cual es parte de nuestro objetivo principal para lo que queremos lograr mas adelante , ejecutamos la interpolación por kriging       ordinario.  


```{r}
k <- krige(formula = a1998log~1, locations = pre1998, newdata = grd, model = f98_m2)
k
plot(k)
summary(exp(as.vector(k$var1.pred)))
```


### Isoyetas

```{r}
plot(raster::raster('kriging.tif'))
plot(raster::rasterToContour(exp(raster::raster('kriging.tif')), levels =seq(600,3000,100)), add=T)
```


### Representacion del Objeto.


```{r}
ggplot() +
  geom_stars(data = k, aes(fill = exp(var1.pred), x = x, y = y)) + 
  scale_fill_gradient(low="#deebf7", high="#3182bd", trans='log1p') +
  geom_sf(data = st_cast(prov, "MULTILINESTRING")) +
  geom_sf(data = pre1998) +
  geom_sf_text(data = prov, aes(label=TOPONIMIA), check_overlap = T, size = 2) +
  theme_bw()
```


### Discusión o Conclusiones

Mediante el procedimiento utilizado para hacer los análisis de datos puntuales y geo estadística, aprendimos a modelizar variogramas muestrales visualizando el comportamiento de homogeneidad de los datos de precipitación para el año 1998.
generamos el kriging ordinario para luego obtener una superficie continua. la cual nos da la posibilidad de crear un mapa de curvas de lluvias o mejor dicho mapa de isoyetas. al final de este proyecto pudimos.
 


### Información de Soporte

Códigos, procedimientos de la clase de superficie continua del profesor José Ramón Martínez Batlle.


\ldots

### Referencias

ISAAKS-SRIVASTAVA-una-introduccion-a-la-geoestadistica-aplicada.pdf
MAS-analisis-espacial-con-R-usa-R-como-un-sistema-informacion-geografia-.pdf.
Capa de division de Provincia de La ONE. (Oficina Nacional de Estadisticas)
Datos de lluvia ONAMET.(Oficina Nacional de Meteorologia)

# PARTE III 

## Modelización de Datos Espaciales.


### Introducción.


Los datos son, como ya sabemos, una parte imprescindible del SIG, ya que sin ellos las aplicaciones SIG y los restantes elementos que se encuentran en torno a estas no tienen utilidad alguna. Necesitamos conocer el área geográfica que estudiamos en un SIG (es decir, tener datos sobre ella), para así poder proceder a dicho estudio.

No obstante, convertir esa área geográfica y la información acerca de ella en un dato susceptible de ser incorporado a un SIG no resulta una tarea sencilla. Desde los orígenes de los SIG, una de las preocupaciones principales ha sido la de representar de la mejor manera posible toda la información que podemos extraer de una zona geográfica dada, de tal modo que pueda almacenarse y analizarse en el entorno de un SIG. Este proceso de representación, que ya desde el inicio planteaba problemas a los creadores de los primeros SIG, ha sido el responsable en gran medida de la arquitectura y forma de los SIG actuales, y a él se debe en buena parte el desarrollo que han experimentado tanto los SIG en sí como las disciplinas afines.


### Metodología para la Modelización Espacial


La metodologia de este proyecto es la modelización de datos espaciales, establecimiento un modelo geográfico. Es decir, un modelo conceptual de la realidad geográfica y su comportamiento.
Iniciamos con la carga de paquetes en este caso solo usaremos el source('lisaclusters.R') ya que las demás librerías esta cargadas en la parte I.
Seleccionamos nuestra variable de estudio la cual es (Categoría Ocupacional: Cesante), y las demás variables que entraran como comparación en este análisis.
Realizamos una serie de análisis como vecindad por contigüidad, ponderadores espaciales, conversión a porcentajes y pruebas estadísticas.
También durante este análisis evaluaremos la autocorrelación espacial global usaremos los criterios de shapiro wilk, Breusch-Pagan test y el I de Moran. crearemos un modelo lineal, utilizando la versión original de la variable, analizaremos el criterio de homocedasticidad.
Interpretaremos los resultados y al final nuestras conclusiones 



### *Script* reproducible

### Paquetes


* Como ya cargamos los paquetes `sf`, la colección `tidyverse` y los paquetes `spdep`, `lmtest`, `tmap` y `RColorBrewer` en la primera parte de este proyecto, solo queda cargar el paquete `lisacluster.R`


```{r}
source('lisaclusters.R')
```

### Datos 

* Exploremos la asociación de una variable de la Encuesta Nacional de viviendas por personas "vivpersgeom_sf" (`.RDS`), asignándolo al objeto `al19`. tenemos un campo mediante el cual podemos hacer un enlace con la tabla, este seria el campo ENLACE, la misma se encuentra dividida por provincias y municipios.  también usaremos la capa geométrica (`.gpkg`) asignándola al objeto `mun`, y la misma será unida al objeto `ad19`. Ambas se encuentran en la carpeta `data`. verificaremos si hay inconsistencia en el código entre ambas fuentes. 



```{r}
setwd("~/unidad-0-asignacion-99-mi-proyecto-Adalbertogp2020/material-de-apoyo-master/data")
munad19 <- readRDS('vivpersgeom_sf.RDS')
```


### Transformacion de los Datos.


```{r}
crsdestino <- 32619
munutm <- munad19 %>% st_transform(crs = crsdestino)
munutm
```


### Selección de Variables


Para nuestro proyecto exploraremos el grado de asociación entre las diferentes variables de la tabla viviendas por personas y la haremos a nivel municipal como una forma de simplificar los cálculos, nuestra variable dependiente será la Categoría Ocupacional: Cesante.

Población total
Categoría Ocupacional: Cesante
Tipo de vivienda: Casa independiente
Tipo de vivienda: Apartamento
Sexo: Hombres
Sexo: Mujeres
Cantidad Cuartos tiene la vivienda: 1
Cantidad Hogares en la vivienda: 1
Acceso a las viviendas del segmento: Calle asfaltadas
Acceso a las viviendas del segmento: Callejon
Principal medio de transporte utilizado por hogares del segmento: Guagua publica
Cual es la relacion o parentesco con la jefa o el jefe del hogar: No pariente


### Carga de Datos


Primero hacemos las selecciones correspondientes, cargamos el archivo objetos_para_modelizacion.RData y las variables, le atribuimos nombres cortos conservando el campo ENLACE:

```{r}
Magdal <- munutm %>% dplyr::select(
  POBLACIONTOTAL = `Población total`,
  CATEGOCUPCESANTE = `Categoría Ocupacional: Cesante`,
  ENLACE = ENLACE,
  TOPONIMIA = TOPONIMIA,
  TIPOVIVIENDA = `Tipo de vivienda: Casa independiente`,
  TIPOAPARTAMENTO = `Tipo de vivienda: Apartamento`,
  SEXOHOMBRE = `Sexo: Hombres`,
  SEXOMUJERE = `Sexo: Mujeres`,
  CANTIDADCUARTOS = `Cantidad Cuartos tiene la vivienda: 1`,
  CANTIDADHOGARES = `Cantidad Hogares en la vivienda: 1`,
  ACCESOVIVIENDA = `Acceso a las viviendas del segmento: Calle asfaltadas`,
  ACCESOCALLEJON = `Acceso a las viviendas del segmento: Callejón`,
  TRANSPORTEPRINCIPAL = `Principal medio de transporte utilizado por hogares del segmento: Guagua pública`,
  PARENTESCO = `Cuál es la relación o parentesco con la jefa o el jefe del hogar: No pariente` )
```

### Vecindad por Contigüidad


```{r}
Magdal.sp <- as_Spatial(Magdal)
row.names(Magdal.sp) <- as.character(Magdal.sp$TOPONIMIA)
Magdal.sp
Magdal.nb <- poly2nb(Magdal.sp, queen=TRUE)
summary(Magdal.nb)
```

Conversion a un objeto sf.


```{r}
plot(Magdal.sp, border="grey", lwd=0.5)
plot(Magdal.nb, coordinates(Magdal.sp), add=T)
```


### Ponderadores Espaciales


```{r}
Magdal.w.W <- nb2listw(Magdal.nb)
Magdal.w.W
Magdal.w.B <- nb2listw(Magdal.nb, style = 'B')
Magdal.w.B
```


### Conversion a Porcentajes


*Con la función `tidyverse`, generamos una columna de porcentaje respecto del tamaño de la muestra a nivel municipal (columna `Población total`). le Pondremos por nombre `adavariable_pct`. luego Haremos una transformada a partir de la anterior, y le pondremos el nombre `de la muestra a nivel municipal (columna `Población total`). le Pondremos por nombre `adavariable_pct_log`. El objeto `sf` resultante se asignará a `Magdal_adavar_sf`


```{r}
Magdal_adavar <- Magdal %>%
  st_centroid() %>% 
  select(ENLACE, CATEGOCUPCESANTE, POBLACIONTOTAL) %>% 
  mutate('PCT_CATEGOCUPCESANTE' = CATEGOCUPCESANTE/POBLACIONTOTAL*100,
         'PCT_CATEGOCUPCESANTE_LOG' = log1p(PCT_CATEGOCUPCESANTE/POBLACIONTOTAL*100),
         x=unlist(map(geom,1)),
         y=unlist(map(geom,2))) %>%
  st_drop_geometry()
Magdal_adavar_sf <- Magdal %>%
  inner_join(Magdal_adavar, by = 'ENLACE') %>% 
  dplyr::select(contains('PCT_CATEGOCUPCESANTE'), x, y, ENLACE, TOPONIMIA)
```

### Creacion de un Mapa usando la Funcion tmap.


```{r}
p1 <- tm_shape(Magdal_adavar_sf) +
  tm_fill(col = "PCT_CATEGOCUPCESANTE", style = 'jenks', palette = brewer.pal(9, name = 'Reds')) +
  tm_borders(lwd = 0.5)
p2 <- tm_shape(Magdal_adavar_sf) +
  tm_fill(col = "PCT_CATEGOCUPCESANTE_LOG", style = 'jenks',
          palette = brewer.pal(9, name = 'Reds'), midpoint = NA) +
  tm_borders(lwd = 0.5)
tmap_arrange(p1, p2)
```


* Analizando el supuesto de normalidad de nuestra variable, en su versión original y en su version transformada, mediante el gráfico cuantilar normal y la prueba de *Shapiro-Wilk*.


```{r}
Magdal_adavar_sf %>% st_drop_geometry() %>%
  gather(variable, valor, -(x:TOPONIMIA)) %>%
  ggplot() + aes(sample=valor) +
  stat_qq() + stat_qq_line() + theme_bw() +
  theme(text = element_text(size = 14)) +
  facet_wrap(~variable, scales = 'free')
```


```{r}
Magdal_adavar_sf %>% st_drop_geometry() %>%
  gather(variable, valor, -(x:TOPONIMIA)) %>% group_by(variable) %>%
  summarise(prueba_normalidad=shapiro.test(valor)$p.value)
```


### Variable Original


```{r}
Magdal_adavar_sf %>% lm(PCT_CATEGOCUPCESANTE~ x, .) %>% plot(3)
Magdal_adavar_sf  %>% lm(PCT_CATEGOCUPCESANTE~ y, .) %>% plot(3)
```


### Prueba Breusch-Pagan


```{r}
Magdal_adavar_sf %>% lm(PCT_CATEGOCUPCESANTE~ x, .) %>% bptest()
Magdal_adavar_sf %>% lm(PCT_CATEGOCUPCESANTE~ y, .) %>% bptest()
```


### Medidas de Autocorrelación Espacial

I de Moran global


```{r}
attr(Magdal.w.W$neighbours, "region.id")
Magdal_adavar_sf$TOPONIMIA
match(attr(Magdal.w.W$neighbours, "region.id"), Magdal_adavar_sf$TOPONIMIA)==1:155
```

### La Autocorrelación Espacial Global


```{r}
(gmoranw <- moran.test(x = Magdal_adavar_sf$`PCT_CATEGOCUPCESANTE`, listw = Magdal.w.W))

```

* Interpreta el resultado de la comprobación anterior aquí:


1-Para los pesos estandarizados=el valor de *p* obtenido fue menor de 0.05, hay evidencia preliminar para rechazar la hipótesis nula de " hay autocorrelación espacial", y por lo tanto concluir que "si hay autocorrelación espacial de la variable Porcentaje de personas con categoría ocupacional cesante".

I de Moran local


```{r}
  moran.plot(x = Magdal_adavar_sf$PCT_CATEGOCUPCESANTE, listw = Magdal.w.W)
```


### Lisamap


```{r}
lisamap(objesp = Magdal_adavar_sf,
        var = 'PCT_CATEGOCUPCESANTE',
        pesos = Magdal.w.W,
        tituloleyenda = 'Significancia\n("x-y", léase\ncomo "x"\nrodeado de "y"',
        leyenda = T,
        anchuratitulo = 50,
        tamanotitulo = 16,
        fuentedatos = 'ONE,2012',
        titulomapa = paste0('Clusters LISA de respuestas a la pregunta:\n"','Porcentaje de personas en Categoría Ocupacional: Cesante', '"'))
```


### Interpretación de Resultados


1-HAY Un patrón de varias provincias coloreadas de rojo se atribuye a "efecto de contagio" importante. esto significa que hay autocorrelación espacial local 

2-las las demás provincias se observa el gris significa que no hay autocorrelación espacial local

3-En las demás provincias se observa el gris significa que no hay autocorrelación espacial local



### Modelización



```{r}
POB19 <- Magdal %>% 
  mutate_each(funs(PCT=round(./POBLACIONTOTAL,4)*100), -ENLACE, -TOPONIMIA, -geom) %>% select(matches('_PCT$'))
```


Modelo lineal común, utilizando la version original de la variables, analizemos el criterio de homocedasticidad:


```{r}
Modlin <-POB19 %>% st_drop_geometry() %>% lm(CATEGOCUPCESANTE_PCT ~ ., data = .)
POB19 %>% summary
Modlin %>% bptest #Breusch-Pagan test
```

Interpretacion.

No es homocedastico


### Modelo Espacial Autorregresivo


```{r}
sar <- POB19 %>% select(contains('_PCT')) %>%
  st_drop_geometry() %>%
  spautolm(
    formula = CATEGOCUPCESANTE_PCT ~ .,
    data = .,
    listw = Magdal.w.W)
summary(sar)
```


```{r}
sar2 <- POB19 %>% select(contains('_PCT')) %>%
  st_drop_geometry() %>%
  spautolm(
    formula = CATEGOCUPCESANTE_PCT ~ TRANSPORTEPRINCIPAL_PCT + TIPOAPARTAMENTO_PCT ,
    data = .,
    listw =  Magdal.w.W)
summary(sar2)
```


### Conclusiones


Los datos son, como ya sabemos, una parte imprescindible del SIG, ya que sin ellos las aplicaciones SIG y los restantes elementos que se encuentran en torno a estas no tienen utilidad alguna. Necesitamos conocer el área geográfica que estudiamos en un SIG (es decir, tener datos sobre ella), para así poder proceder a dicho estudio.

De acuerdo con nuestra variable dependiente de la "Categoría Ocupacional: Cesante"
los resultados obtenidos mediante las pruebas de autocorrelacion pudimos probar que en algunas provincias hay un patrón de relleno rojo y azul de acuerdo con el mapa lisa clúster, significa que existe autocorrelación espacial global y un efecto de contagio importante en el desempleo.
no obstante en  las demás provincias se observa el color gris significa que no hay autocorrelación espacial global 

Continuando con el análisis de modelización espacial global mediante las pruebas sar.
solo dos variables resultaron significativas con relación a la Categoría Ocupacional: Cesante 

TRANSPORTEPRINCIPAL_PCT -0.0105590  0.0058618 -1.8013  0.07165

Con un valor cercano al 0.05 y en negativo la variable "Principal medio de transporte utilizado por hogares del segmento: Guagua pública" nos indica que cuando la categoría ocupacional cesante disminuye, el transporte principal utilizado aumenta. 

TIPOAPARTAMENTO_PCT       0.0587764  0.0300015  1.9591  0.05010

Con un valor de 0.05 y en positivo la variable "Tipo de vivienda: Apartamento" nos indica que cuando la categoría ocupacional cesante aumenta el Tipo de vivienda Apartamento disminuye



### Discusión 

Los encargados de la planificación territorial se enfrentan al problema de manejar una gran cantidad de información
espacial que les permita cumplir su labor en forma oportuna y satisfactoria. La presente investigación plantea una metodología
basada en la integración de la Geomática y las Técnicas de Evaluación Multicriterio (EMC) para obtener un modelo de
capacidad de acogida que facilite la correlación de muchas variables que están presente en el día a día de nuestra sociedad específicamente para la clase de nivel Bajo. en nuestro análisis usamos la variable original. mediante la prueba de Shapiro-Wilk,  y el I de moran pudimos establecer los criterios para cada prueba descritos más arriba.



\ldots

### Información de Soporte

Códigos, procedimientos de la clase de Vecindad, autocorrelacion espacial y modelización del profesor José Ramón Martínez Batlle.


\ldots

### Referencias

BIVAND-PEBESMA-GOMEZ-RUBIO-analisis-espacial-aplicado-con-R.pdf
ISAAKS-SRIVASTAVA-una-introduccion-a-la-geoestadistica-aplicada.pdf
MAS-analisis-espacial-con-R-usa-R-como-un-sistema-informacion-geografia-.pdf
Capa de division de Provincia de La ONE. (Oficina Nacional de Estadisticas)
Encuesta vivpersgeom 2011 de la ONE.(Oficina Nacional de Estadisticas)
Capa de ProvCenso2010 de la ONE.(Oficina Nacional de Estadisticas).



